= TL;DR RLHF

[IMPORTANT]
====
This is a document with notes(mostly extractive) on the topic of RLHF. Check the full resources in the literature section below.
==== 

== *_RLHF -- Reinforcement Learning from Human Feedback._*

== Development process of LLMs:

1. Model is trained on various indiscriminate data;
2. Then it is fine-tuned on higher quality data. The result of these actions is called SFT -- supervised fine-tuning;
3. RLHF is making model more customer appropriate.

== Language model development basics:
*_Training data_* -- the text given to the model to learn from. Tokens are extracted from training data.
*_Token_* -- can refer to a word, a character, or a part of a word (like -tion), depending on the language model.
Token is a part of a  vocabulary which language model uses.
Given a text prompt, model predicts the next most probable token, therefore it is trained for completion.

There exists wide variety of completion tasks: summarization, translation, paraphrasing, coding etc.

*_Language models are only as good as their training data._*
Language model can be fine-tuned on instructions or dialogues.

=== Data bottleneck for pretraining
We are running out of training data, LLMs are trained on 1 trillion tokens which are equivalent to 15 million books.

== Supervised fine-tuning (SFT) in more detail
The goal of SFT is to optimize the pretrained model to generate the responses that users are looking for.
*_Demonstration data_* -- the examples of expected responses which follow the format (prompt, response) and are used either for model fine-tuning (produces better results) or training from scratch.
Demonstration data is commonly generated by humans.

== RLHF
Demonstration data doesn’t tell the model how good or how bad a response is.

*_Core idea_*:

1. Train a reward model to act as a scoring function.
2. Optimize LLM to generate responses for which the reward model will give high scores.

_Why it works:_

1. Diversity -- SFT matches output with expected responses, unexpected are marked as wrong;
2. Negative feedback -- RL allows both positive and negative scores;
3. Hallucination -- [add something here] RLHF HELPS!

*_Reward model_* -- output a score for a pair of (prompt, response).
To train this we ask labelers to compare two responses and decide which one is better.
*_Comparison data_* -- data produced during labeling process and looks like this: (prompt, winning_response, losing_response).
*_Challenge_*: Human preferences are diverse and impossible to capture in a single mathematical formulation plus how do you train the model to give concrete scores.

People have experimented with different ways to initialize an RM: e.g. training an RM from scratch or starting with the SFT model as the seed. Starting from the SFT model seems to give the best performance. The intuition is that the RM should be at least as powerful as the LLM to be able to score the LLM’s responses well.


=== Fine-tuning the reward model:
Use https://openai.com/research/openai-baselines-ppo[Proximal Policy Optimization (PPO)] to train the SFT model to generate output responses that will maximize the scores by the RM.
The reward is used to update the policy using PPO.

Prompts are randomly selected from a distribution, each prompt is the input into the LLM model to get back a response, which is given a score by the RM.

https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence[*_Kullback–Leibler divergence_*] -- type of statistical distance: a measure of how one probability distribution P is different from a second, reference probability distribution Q.The model resulting from this phase should not stray too far from the model resulting from the SFT phase.

== Hallucination

*_Hallucination_* happens when model makes stuff up. 

Two hypotheses on why models hallucinate:

1. “lack the understanding of the cause and effect of their actions” (DeepMind,  https://arxiv.org/abs/2110.10819#deepmind[Pedro A. Ortega et al])
2. mismatch between the LLM’s internal knowledge and the labeler’s internal knowledge (OpenAI, John Schulman)

A couple of solutions (by Schulman)

1. Verification: asking the LLM to explain (retrieve) the sources where it gets the answer from.
2. RL -- punish model more for making things up. 

Making LLMs respond concisely also seems to help with hallucination – the fewer tokens LLMs have to generate, the less chance they have to make things up.

= Literature
1. https://huyenchip.com/2023/05/02/rlhf.html[RLHF: Reinforcement Learning from Human Feedback -- Chip Huyen]
2. https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2[State of GPT -- Andrej Karpathy]
3. https://youtu.be/4W3MQkApH9Y[RLHF Intro: from Zero to Aligned Intelligent Systems -- Igor Kotenkov] 
